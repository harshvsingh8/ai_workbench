{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2746c71",
   "metadata": {},
   "source": [
    "## GPT Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "58081875",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,     # Vocabulary size\n",
    "    \"context_length\": 1024,  # Context length\n",
    "    \"emb_dim\": 768,          # Embedding dimension\n",
    "    \"n_heads\": 12,           # Number of attention heads\n",
    "    \"n_layers\": 12,          # Number of layers\n",
    "    \"drop_rate\": 0.1,        # Dropout rate\n",
    "    \"qkv_bias\": False        # Query-Key-Value bias\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26d57a6b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'batch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 8\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msys\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtiktoken\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[43mbatch\u001b[49m, \u001b[38;5;28mlist\u001b[39m):\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(batch[\u001b[38;5;241m0\u001b[39m], torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[1;32m     10\u001b[0m         batch \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack(batch)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'batch' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "import torch\n",
    "import re\n",
    "import sys\n",
    "import tiktoken\n",
    "\n",
    "\n",
    "if isinstance(batch, list):\n",
    "    if isinstance(batch[0], torch.Tensor):\n",
    "        batch = torch.stack(batch)\n",
    "    else:\n",
    "        batch = torch.tensor(batch, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef4a411c",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392e36d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([6109, 3626, 6100,  345]), tensor([6109, 1110, 6622,  257])]\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "batch = []\n",
    "txt1 = \"Every effort moves you\"\n",
    "txt2 = \"Every day holds a\"\n",
    "\n",
    "batch.append(torch.tensor(tokenizer.encode(txt1)))\n",
    "batch.append(torch.tensor(tokenizer.encode(txt2)))\n",
    "\n",
    "print(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5163171f",
   "metadata": {},
   "source": [
    "## Testing DummyGPTModel - test 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "762330df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([2, 4, 50257])\n",
      "tensor([[[-0.9289,  0.2748, -0.7557,  ..., -1.6070,  0.2702, -0.5888],\n",
      "         [-0.4476,  0.1726,  0.5354,  ..., -0.3932,  1.5285,  0.8557],\n",
      "         [ 0.5680,  1.6053, -0.2155,  ...,  1.1624,  0.1380,  0.7425],\n",
      "         [ 0.0447,  2.4787, -0.8843,  ...,  1.3219, -0.0864, -0.5856]],\n",
      "\n",
      "        [[-1.5474, -0.0542, -1.0571,  ..., -1.8061, -0.4494, -0.6747],\n",
      "         [-0.8422,  0.8243, -0.1098,  ..., -0.1434,  0.2079,  1.2046],\n",
      "         [ 0.1355,  1.1858, -0.1453,  ...,  0.0869, -0.1590,  0.1552],\n",
      "         [ 0.1666, -0.8138,  0.2307,  ...,  2.5035, -0.3055, -0.3083]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import dummy_gpt_model\n",
    "importlib.reload(dummy_gpt_model)\n",
    "from dummy_gpt_model import DummyGPTModel\n",
    "\n",
    "torch.manual_seed(123)\n",
    "model = DummyGPTModel(GPT_CONFIG_124M)\n",
    "batch = torch.stack(batch) if isinstance(batch, list) else batch\n",
    "logits = model(batch)\n",
    "print(\"Output shape:\", logits.shape)\n",
    "print(logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f51734a5",
   "metadata": {},
   "source": [
    "## Layer Normalization\n",
    "\n",
    "Layer Normalization is a crucial component in modern transformer architectures like GPT for several key reasons.\n",
    "\n",
    "In short, **it stabilizes the training process and helps the model learn more efficiently.**\n",
    "\n",
    "Hereâ€™s a breakdown of why it's so important:\n",
    "\n",
    "### 1. Stabilizes Hidden State Values (Activations)\n",
    "\n",
    "As data passes through many layers of a deep network, the values can become very large or very small. This can lead to:\n",
    "- **Vanishing Gradients**: Activations become tiny, causing the gradients used for learning to become zero. The model stops learning.\n",
    "- **Exploding Gradients**: Activations become huge, causing gradients to become massive (`NaN` or `inf`). The model's weights are destroyed, and training fails.\n",
    "\n",
    "Layer Normalization combats this by **rescaling the activations within each layer** for every single training example. It forces the outputs of a layer to have a mean of 0 and a standard deviation of 1, keeping them in a controlled, \"well-behaved\" range.\n",
    "\n",
    "### 2. Speeds Up Training\n",
    "\n",
    "By keeping activations stable, Layer Norm allows you to use **higher learning rates** without the training process becoming unstable. A higher learning rate means the model can learn faster, significantly reducing training time.\n",
    "\n",
    "### 3. Reduces Sensitivity to Initialization\n",
    "\n",
    "Without normalization, the initial weights of the model must be chosen very carefully. A bad initialization can cause the network to fail from the start. Layer Norm makes the model much more robust to the choice of initial weights.\n",
    "\n",
    "### How It Works in Your Code\n",
    "\n",
    "In your `dummy_gpt_model.py`, Layer Normalization is planned in two key places:\n",
    "\n",
    "1.  **Inside the Transformer Block**: As the `TODO` comments in dummy_gpt_model.py suggest, it's applied *before* the self-attention and feed-forward sub-layers. This is known as the **\"Pre-LN\"** architecture, which is very common and effective. It ensures the inputs to these critical components are always well-scaled.\n",
    "\n",
    "2.  **Before the Final Output**: The dummy_gpt_model.py layer normalizes the output of the last transformer block before it's fed to the final linear layer (`out_head`) to produce logits. This provides one last stabilization step before the final prediction.\n",
    "\n",
    "### LayerNorm vs. BatchNorm\n",
    "\n",
    "You might have heard of Batch Normalization. The key difference is the dimension they normalize over:\n",
    "- **BatchNorm**: Normalizes across the **batch dimension**. It calculates one mean/variance for the entire batch. This works well for CNNs but can be tricky for variable-length sequences in LMs.\n",
    "- **LayerNorm**: Normalizes across the **feature dimension** for *each individual sequence*. This makes it independent of the batch size and sequence length, which is ideal for transformers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db4f2845",
   "metadata": {},
   "source": [
    "## Layer Normalization - Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f35b99f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- set up tensor sample for showing mean and variance\n",
      "tensor([[0.2260, 0.3470, 0.0000, 0.2216, 0.0000, 0.0000],\n",
      "        [0.2133, 0.2394, 0.0000, 0.5198, 0.3297, 0.0000]],\n",
      "       grad_fn=<ReluBackward0>)\n",
      "--- mean and variance\n",
      "Mean:\n",
      " tensor([[0.1324],\n",
      "        [0.2170]], grad_fn=<MeanBackward1>)\n",
      "Variance:\n",
      " tensor([[0.0231],\n",
      "        [0.0398]], grad_fn=<VarBackward0>)\n",
      "--- Normalizing layer output\n",
      "Normalized layer output:\n",
      " tensor([[ 0.6159,  1.4126, -0.8719,  0.5872, -0.8719, -0.8719],\n",
      "        [-0.0189,  0.1121, -1.0876,  1.5173,  0.5647, -1.0876]],\n",
      "       grad_fn=<DivBackward0>)\n",
      "Mean:\n",
      " tensor([[    0.0000],\n",
      "        [    0.0000]], grad_fn=<MeanBackward1>)\n",
      "Variance:\n",
      " tensor([[1.0000],\n",
      "        [1.0000]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "torch.manual_seed(123)\n",
    "torch.set_printoptions(sci_mode=False)\n",
    "\n",
    "print(\"--- set up tensor sample for showing mean and variance\")\n",
    "batch_example = torch.randn(2, 5)\n",
    "layer = nn.Sequential(nn.Linear(5, 6), nn.ReLU())\n",
    "out = layer(batch_example)\n",
    "print(out)\n",
    "\n",
    "print(\"--- mean and variance\")\n",
    "mean = out.mean(dim=-1, keepdim=True)\n",
    "var = out.var(dim=-1, keepdim=True)\n",
    "\n",
    "print(\"Mean:\\n\", mean)\n",
    "print(\"Variance:\\n\", var)\n",
    "\n",
    "print(\"--- Normalizing layer output\")\n",
    "out_norm = (out - mean) / torch.sqrt(var)\n",
    "mean = out_norm.mean(dim=-1, keepdim=True)\n",
    "var = out_norm.var(dim=-1, keepdim=True)\n",
    "print(\"Normalized layer output:\\n\", out_norm)\n",
    "print(\"Mean:\\n\", mean)\n",
    "print(\"Variance:\\n\", var)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "499d0c78",
   "metadata": {},
   "source": [
    "## LayerNorm - Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "29ddf671",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- with unbiased = False (default)\n",
      "Mean:\n",
      " tensor([[    -0.0000],\n",
      "        [     0.0000]], grad_fn=<MeanBackward1>)\n",
      "Variance:\n",
      " tensor([[1.2499],\n",
      "        [1.2500]], grad_fn=<VarBackward0>)\n",
      "--- with unbiased = True\n",
      "Mean:\n",
      " tensor([[    -0.0000],\n",
      "        [     0.0000]], grad_fn=<MeanBackward1>)\n",
      "Variance:\n",
      " tensor([[1.0000],\n",
      "        [1.0000]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import layer_norm\n",
    "\n",
    "# Needed only if we are changing LayerNorm file.\n",
    "import importlib\n",
    "importlib.reload(layer_norm)\n",
    "\n",
    "torch.manual_seed(123)\n",
    "torch.set_printoptions(sci_mode=False)\n",
    "\n",
    "batch_example = torch.randn(2, 5)\n",
    "\n",
    "print(\"--- with unbiased = False (default)\")\n",
    "ln = layer_norm.LayerNorm(emb_dim=5)\n",
    "out_ln = ln(batch_example)\n",
    "\n",
    "print(\"Mean:\\n\", out_ln.mean(dim=-1, keepdim=True))\n",
    "print(\"Variance:\\n\", out_ln.var(dim=-1, keepdim=True))\n",
    "\n",
    "print(\"--- with unbiased = True\")\n",
    "ln_bias = layer_norm.LayerNorm(emb_dim=5, unbiased=True)\n",
    "out_ln_bias = ln_bias(batch_example)\n",
    "print(\"Mean:\\n\", out_ln_bias.mean(dim=-1, keepdim=True))\n",
    "print(\"Variance:\\n\", out_ln_bias.var(dim=-1, keepdim=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e6801ecf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of normalized output (before scale and shift):\n",
      " tensor([[    -0.0000],\n",
      "        [     0.0000]])\n",
      "Variance of normalized output (before scale and shift):\n",
      " tensor([[1.0000],\n",
      "        [1.0000]])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Re-using batch_example from the previous cell\n",
    "ln = layer_norm.LayerNorm(emb_dim=5)\n",
    "\n",
    "# Manually compute the normalized output without scale and shift\n",
    "mean = batch_example.mean(dim=-1, keepdim=True)\n",
    "var = batch_example.var(dim=-1, keepdim=True, unbiased=False)\n",
    "norm_x = (batch_example - mean) / torch.sqrt(var + ln.eps)\n",
    "\n",
    "print(\"Mean of normalized output (before scale and shift):\\n\", norm_x.mean(dim=-1, keepdim=True))\n",
    "print(\"Variance of normalized output (before scale and shift):\\n\", norm_x.var(dim=-1, keepdim=True, unbiased=False))\n",
    "\n",
    "# Note: The variance is calculated with unbiased=False to match PyTorch's LayerNorm implementation for variance calculation.\n",
    "# Using unbiased=False for the final variance check confirms that the normalization step itself produces a variance of 1.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python(ai_workbench)",
   "language": "python",
   "name": "ai_workbench"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
